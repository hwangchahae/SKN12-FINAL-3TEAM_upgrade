import json
import os
import sys
import torch
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    __version__ as transformers_version
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging
import gc

# ëŸ°íŒŸ í™˜ê²½ì„ ìœ„í•œ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.getenv('PROJECT_ROOT', os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from meeting_analysis_prompts import generate_meeting_analysis_system_prompt, generate_meeting_analysis_user_prompt

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# CUDA ë©”ëª¨ë¦¬ ì„¤ì •
if torch.cuda.is_available():
    # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:512"
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()

# Flash Attention ê°€ìš©ì„± í™•ì¸
try:
    import flash_attn
    FLASH_ATTN_AVAILABLE = True
    logger.info("Flash Attention ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    FLASH_ATTN_AVAILABLE = False
    logger.warning("Flash Attention ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ attention ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©")

# packaging ëª¨ë“ˆ í™•ì¸
try:
    from packaging import version
except ImportError:
    logger.error("packaging ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install packaging")
    sys.exit(1)

def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ëª¨ë‹ˆí„°ë§"""
    if torch.cuda.is_available():
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        before_memory = torch.cuda.memory_allocated() / 1024**3
        
        # ìºì‹œ ì •ë¦¬
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ í™•ì¸
        after_memory = torch.cuda.memory_allocated() / 1024**3
        freed_memory = before_memory - after_memory
        
        logger.info(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (í•´ì œ: {freed_memory:.2f}GB, í˜„ì¬: {after_memory:.2f}GB)")

class TtalkkacDatasetConverter:
    """ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì™€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œë¥¼ ë³€í™˜"""
    
    def __init__(self):
        # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        self.system_prompt = generate_meeting_analysis_system_prompt()
        # user_prompt_templateì€ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ ë©”ì„œë“œì—ì„œ ì²˜ë¦¬

    def chunk_text(self, text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì—¬ ë‚˜ëˆ„ê¸° (ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œì™€ ë™ì¼í•œ ë°©ì‹)"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                chunk = text[start:]
            else:
                chunk = text[start:end]
                
                # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                break_point = max(last_period, last_newline)
                
                if break_point > start + chunk_size // 2:
                    chunk = text[start:break_point + 1]
                    end = break_point + 1
            
            chunks.append(chunk.strip())
            
            if end >= len(text):
                break
                
            start = end - overlap
        
        return chunks

    def load_gold_standard_data(self, results_dir: str = None) -> List[Dict[str, Any]]:
        """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ê²°ê³¼ í´ë”ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        if results_dir is None:
            results_dir = os.path.join(PROJECT_ROOT, "ttalkkac_gold_standard_results_output")
        results_path = Path(results_dir)
        data = []
        
        if not results_path.exists():
            logger.error(f"ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")
            return []
        
        # ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ result.json íŒŒì¼ë“¤ ìŠ¤ìº”
        for folder in results_path.iterdir():
            if folder.is_dir() and folder.name.startswith(('train_', 'val_')):
                result_file = folder / "result.json"
                if result_file.exists():
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            item = json.load(f)
                        data.append(item)
                        logger.info(f"ë¡œë“œ: {folder.name}")
                    except Exception as e:
                        logger.error(f"{folder.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        logger.info(f"ì´ {len(data)}ê°œ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return data

    def load_meeting_content(self, source_dir: str, meeting_folder: str) -> str:
        """ì›ë³¸ íšŒì˜ ë‚´ìš© ë¡œë“œ (batch_triplet_resultsì—ì„œ)"""
        # OS ë…ë¦½ì  ê²½ë¡œ ì²˜ë¦¬                                                                                â”‚ â”‚
        source_path = Path(source_dir)  

        # result_ ì ‘ë‘ì‚¬ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if meeting_folder.startswith("result_"):
            meeting_path = source_path / meeting_folder / "05_final_result.json"
        else:
            meeting_path = source_path / f"result_{meeting_folder}" / "05_final_result.json"   
        
        if not meeting_path.exists():
            logger.warning(f"íšŒì˜ë¡ íŒŒì¼ ì—†ìŒ: {meeting_path}")
            return ""
        
        try:
            with open(meeting_path, 'r', encoding='utf-8') as f:
                meeting_data = json.load(f)
            
            # ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ íšŒì˜ ë‚´ìš© êµ¬ì„±
            meeting_text = ""
            for item in meeting_data:
                timestamp = item.get('timestamp', 'Unknown')
                speaker = item.get('speaker', 'Unknown')
                text = item.get('text', '')
                meeting_text += f"[{timestamp}] {speaker}: {text}\n"
            
            return meeting_text.strip()
        except Exception as e:
            logger.error(f"íšŒì˜ë¡ ë¡œë“œ ì‹¤íŒ¨ {meeting_path}: {e}")
            return ""

    def convert_to_training_format(self, gold_data: List[Dict[str, Any]], 
                                 source_dir: str = None) -> List[Dict[str, str]]:
        if source_dir is None:
            source_dir = os.path.join(PROJECT_ROOT, "batch_triplet_results_input")
        """ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œë¥¼ Qwen3 íŒŒì¸íŠœë‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì²­í‚¹ ì§€ì›)"""
        training_data = []
        
        for item in gold_data:
            try:
                metadata = item.get('metadata', {})
                source_file = metadata.get('source_file', '')
                is_chunk = metadata.get('is_chunk', False)
                
                logger.info(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {item.get('id', 'Unknown')}")
                logger.info(f"   ğŸ“‚ ì²­í‚¹ì—¬ë¶€: {is_chunk}")
                logger.info(f"   ğŸ“„ source_file: {source_file}")
                
                if not source_file:
                    logger.warning(f"âŒ source_file ì—†ìŒ: {item.get('id', 'Unknown')}")
                    continue
                
                # source_fileì—ì„œ ì‹¤ì œ í´ë”ëª… ì¶”ì¶œ - OS ë…ë¦½ì  ì²˜ë¦¬
                source_file_path = Path(source_file)
                
                # ê²½ë¡œ ë¶„ë¦¬ í›„ result_ í´ë”ëª… ì¶”ì¶œ
                parts = source_file_path.parts
                source_folder = None
                
                for part in parts:
                    if part.startswith('result_'):
                        # result_ ì ‘ë‘ì‚¬ ì œê±°í•˜ì—¬ ì‹¤ì œ í´ë”ëª… ì¶”ì¶œ
                        source_folder = part[7:]  # 'result_' ê¸¸ì´ê°€ 7
                        break
                    elif part.startswith(('train_', 'val_')):
                        # train_ ë˜ëŠ” val_ ì ‘ë‘ì‚¬ ì œê±°
                        if part.startswith('train_'):
                            source_folder = part[6:]  # 'train_' ê¸¸ì´ê°€ 6
                        else:
                            source_folder = part[4:]  # 'val_' ê¸¸ì´ê°€ 4
                        break
                
                if not source_folder:
                    # í´ë”ëª…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
                    source_folder = source_file_path.stem
                
                logger.info(f"   ğŸ—‚ï¸  ì¶”ì¶œëœ í´ë”ëª…: {source_folder}")
                logger.info(f"   ğŸ“ Input ê²½ë¡œ: {Path(source_dir) / source_folder / '05_final_result.json'}")
                
                # ì›ë³¸ íšŒì˜ ë‚´ìš© ë¡œë“œ
                full_meeting_content = self.load_meeting_content(source_dir, source_folder)
                if not full_meeting_content:
                    logger.warning(f"âŒ íšŒì˜ ë‚´ìš© ì—†ìŒ: {source_folder}")
                    continue
                
                logger.info(f"   ğŸ“Š ì›ë³¸ íšŒì˜ë¡ ê¸¸ì´: {len(full_meeting_content)}ì")
                
                # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ìƒì„±ê³¼ ë™ì¼í•œ ì²­í‚¹ ì¡°ê±´ ì ìš©
                CHUNK_THRESHOLD = 5000  # ì„¤ì • ê°€ëŠ¥í•œ ìƒìˆ˜ë¡œ ë³€ê²½
                if len(full_meeting_content) > CHUNK_THRESHOLD:
                    # 5000ì ì´ˆê³¼: ì²­í‚¹ ì²˜ë¦¬
                    logger.info(f"   ğŸ“ ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(full_meeting_content)}ì) - ì²­í‚¹ ì²˜ë¦¬")
                    chunks = self.chunk_text(full_meeting_content, chunk_size=CHUNK_THRESHOLD, overlap=512)
                    logger.info(f"   âœ‚ï¸ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
                    
                    # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ IDì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ì¶”ì¶œ
                    item_id = item.get('id', '')
                    if '_chunk_' in item_id:
                        chunk_str = item_id.split('_chunk_')[-1]
                        try:
                            chunk_index = int(chunk_str) - 1  # 1-based â†’ 0-based
                        except ValueError:
                            logger.error(f"   âŒ ì²­í¬ ì¸ë±ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {chunk_str}")
                            continue
                        
                        if chunk_index < len(chunks):
                            meeting_content = chunks[chunk_index]
                            logger.info(f"   âœ… ì²­í¬ ë§¤ì¹­ ì„±ê³µ!")
                            logger.info(f"      - ì‚¬ìš©í•  ì²­í¬: {chunk_index+1}/{len(chunks)}")
                            logger.info(f"      - ì‹¤ì œ ì²­í¬ ê¸¸ì´: {len(meeting_content)}ì")
                        else:
                            logger.error(f"   âŒ ì²­í¬ ì¸ë±ìŠ¤ ì´ˆê³¼: {chunk_index+1} > {len(chunks)}")
                            continue
                    else:
                        # ì²­í¬ ì¸ë±ìŠ¤ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì²­í¬ ì‚¬ìš©
                        meeting_content = chunks[0]
                        logger.info(f"   âš ï¸  ì²­í¬ ì¸ë±ìŠ¤ ì—†ìŒ, ì²« ë²ˆì§¸ ì²­í¬ ì‚¬ìš©")
                else:
                    # 5000ì ì´í•˜: ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    meeting_content = full_meeting_content
                    logger.info(f"   ğŸ“– ì „ì²´ íšŒì˜ë¡ ì‚¬ìš© (ê¸¸ì´: {len(meeting_content)}ì)")
                
                # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                user_message = generate_meeting_analysis_user_prompt(meeting_content)
                
                # ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ì‘ë‹µ ì²˜ë¦¬
                notion_output = item.get('notion_output', {})
                if not notion_output:
                    logger.warning(f"notion_output ì—†ìŒ: {item.get('id', 'Unknown')}")
                    continue
                
                # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                if isinstance(notion_output, str):
                    try:
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°
                        clean_json = notion_output.strip()
                        if clean_json.startswith('```json\n') and clean_json.endswith('\n```'):
                            clean_json = clean_json[8:-4]  # ```json\nê³¼ \n``` ì œê±°
                        elif clean_json.startswith('```\n') and clean_json.endswith('\n```'):
                            clean_json = clean_json[4:-4]  # ```\nê³¼ \n``` ì œê±°
                        
                        notion_output = json.loads(clean_json)
                    except Exception as e:
                        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {item.get('id', 'Unknown')} - {str(e)}")
                        continue
                    
                assistant_response = json.dumps(notion_output, ensure_ascii=False, indent=2)
                
                # Qwen3 ì±„íŒ… í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
                conversation = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant\n{assistant_response}<|im_end|>"
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                quality_metrics = item.get('quality_metrics', {})
                quality_score = quality_metrics.get('final_score', 7.0)
                is_high_quality = quality_metrics.get('is_high_quality', True)
                
                training_data.append({
                    "text": conversation,
                    "metadata": {
                        "id": item.get('id', 'Unknown'),
                        "source": source_folder,
                        "quality_score": quality_score,
                        "is_high_quality": is_high_quality,
                        "is_chunk": is_chunk,
                        "chunk_info": item.get('chunk_info', {}),
                        "dataset_type": "train" if "train_" in str(item.get('id', '')) else "val"
                    }
                })
                
                logger.info(f"ë³€í™˜ ì™„ë£Œ: {item.get('id', 'Unknown')} (í’ˆì§ˆ: {quality_score}/10)")
                
            except Exception as e:
                logger.error(f"ë³€í™˜ ì‹¤íŒ¨ {item.get('id', 'Unknown')}: {e}")
                continue
        
        return training_data

class QwenFineTuner:
    def __init__(self, model_name: str = "Qwen/Qwen3-4B-Thinking-2507"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        
    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
        logger.info(f"ëª¨ë¸ ë¡œë”©: {self.model_name}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_gpu_memory()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        if torch.cuda.is_available():
            initial_memory = torch.cuda.memory_allocated() / 1024**3
            logger.info(f"ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {initial_memory:.1f}GB")
            
            # GPU í”„ë¡œì„¸ìŠ¤ í™•ì¸
            import subprocess
            try:
                result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,used_memory', '--format=csv,noheader'], 
                                      capture_output=True, text=True)
                if result.stdout.strip():
                    logger.warning("ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤:")
                    logger.warning(result.stdout)
            except:
                pass
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_gpu_memory()
        
        # ì¼ë°˜ ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìµœì í™”)
        try:
            # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
            from transformers import BitsAndBytesConfig
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            # ëª¨ë¸ ë¡œë“œ ì„¤ì •
            model_kwargs = {
                "torch_dtype": torch.float16,
                "quantization_config": bnb_config,
                "device_map": "auto",
                "trust_remote_code": True,
                "use_cache": False,
                "low_cpu_mem_usage": True,
            }
            
            # Flash Attention ì„¤ì •
            if FLASH_ATTN_AVAILABLE:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                logger.info("Flash Attention 2 í™œì„±í™”")
            
            logger.info("4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                **model_kwargs
            )
            
            # gradient checkpointing ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                try:
                    self.model.gradient_checkpointing_enable()
                    logger.info("Gradient checkpointing í™œì„±í™”")
                except Exception as e:
                    logger.warning(f"Gradient checkpointing í™œì„±í™” ì‹¤íŒ¨ (4ë¹„íŠ¸ ì–‘ìí™”ì™€ ì¶©ëŒ ê°€ëŠ¥): {e}")
            
        except ImportError as e:
            if "bitsandbytes" in str(e):
                logger.error("âŒ bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install bitsandbytes")
            raise RuntimeError(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ê±°ë‚˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë¸ ë¡œë”© í›„ ë©”ëª¨ë¦¬ í™•ì¸
        if torch.cuda.is_available():
            model_memory = torch.cuda.memory_allocated() / 1024**3
            logger.info(f"ëª¨ë¸ ë¡œë”© í›„ GPU ë©”ëª¨ë¦¬: {model_memory:.1f}GB")
            
        # ëª¨ë¸ í¬ê¸° í™•ì¸
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
        
        logger.info("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
    
    def setup_lora_config(self) -> LoraConfig:
        """LoRA ì„¤ì •"""
        return LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,   # ì›ë˜ ì„¤ì • ìœ ì§€
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # ì „ì²´ ì–´í…ì…˜ ëª¨ë“ˆ
            bias="none",
        )
    
    def prepare_dataset(self, training_data: List[Dict[str, str]], max_length: int = 12000, val_split: float = 0.2):
        """ë°ì´í„°ì…‹ ì¤€ë¹„ ë° í† í¬ë‚˜ì´ì§•"""
        def tokenize_function(examples):
            # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=max_length,
                return_tensors=None,
            )
            
            # labels = input_ids (ìë™íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§)
            tokenized["labels"] = [ids[:] for ids in tokenized["input_ids"]]
            
            return tokenized
        
        # ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        all_data = training_data
        
        logger.info(f"ì „ì²´ ë°ì´í„°: {len(all_data)}ê°œ")
        
        # train/val ë¶„í• 
        import random
        random.seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
        all_data_copy = all_data.copy()
        random.shuffle(all_data_copy)
        all_data = all_data_copy
        split_idx = int(len(all_data) * (1 - val_split))
        train_data = all_data[:split_idx]
        val_data = all_data[split_idx:]
        
        logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ, ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
        
        if len(train_data) == 0:
            logger.error("í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None, None
        
        # Dataset ê°ì²´ ìƒì„±
        train_dataset = Dataset.from_list([{"text": item["text"]} for item in train_data])
        val_dataset = Dataset.from_list([{"text": item["text"]} for item in val_data]) if val_data else None
        
        # í† í¬ë‚˜ì´ì§•
        train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=1, remove_columns=["text"])
        if val_dataset is not None:
            val_dataset = val_dataset.map(tokenize_function, batched=True, batch_size=1, remove_columns=["text"])
        
        return train_dataset, val_dataset
    
    def train(self, train_dataset, val_dataset, output_dir: str = "./qwen3_4B_Thinking_lora_ttalkkac"):
        """LoRA íŒŒì¸íŠœë‹ ì‹¤í–‰"""
        
        # LoRA ì ìš©
        lora_config = self.setup_lora_config()
        self.model = get_peft_model(self.model, lora_config)
        
        # 4ë¹„íŠ¸ ëª¨ë¸ì—ì„œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        self.model.enable_input_require_grads()
        
        # LoRA íŒŒë¼ë¯¸í„° í™•ì¸
        self.model.print_trainable_parameters()
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8 if torch.cuda.is_available() else 4,  # GPU/CPUì— ë”°ë¼ ì¡°ì •
            warmup_steps=50,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=1,  # ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¡œê¹…
            eval_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=100,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            dataloader_num_workers=0,
            remove_unused_columns=False,
            report_to="none",
            push_to_hub=False,
            logging_dir=f"{output_dir}/logs",
            optim="paged_adamw_8bit",  # 8ë¹„íŠ¸ optimizer
            max_grad_norm=0.3,
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            return_tensors="pt"
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer_kwargs = {
            "model": self.model,
            "args": training_args,
            "train_dataset": train_dataset,
            "eval_dataset": val_dataset,
            "data_collator": data_collator,
        }
        
        # Transformers ë²„ì „ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì„ íƒ
        if version.parse(transformers_version) >= version.parse("4.41.0"):
            trainer_kwargs["processing_class"] = self.tokenizer
        else:
            trainer_kwargs["tokenizer"] = self.tokenizer
        
        trainer = Trainer(**trainer_kwargs)
        
        # í•™ìŠµ ì‹¤í–‰
        logger.info("íŒŒì¸íŠœë‹ ì‹œì‘...")
        train_result = trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        trainer.save_state()
        
        # í•™ìŠµ ê²°ê³¼ ì €ì¥
        with open(os.path.join(output_dir, "training_results.json"), "w") as f:
            json.dump({
                "train_runtime": train_result.metrics["train_runtime"],
                "train_samples_per_second": train_result.metrics["train_samples_per_second"],
                "train_steps_per_second": train_result.metrics["train_steps_per_second"],
                "total_flos": train_result.metrics.get("total_flos", 0),
                "train_loss": train_result.metrics["train_loss"],
            }, f, indent=2)
        
        logger.info(f"íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir}")
        return trainer

def main():
    print("=" * 60)
    print("ğŸš€ Ttalkkac Qwen3 LoRA íŒŒì¸íŠœë‹ ì‹œì‘ (4ë¹„íŠ¸ ì–‘ìí™”)")
    print("=" * 60)
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}")
    print(f"ğŸ Python ë²„ì „: {sys.version}")
    print(f"ğŸ”§ PyTorch ë²„ì „: {torch.__version__}")
    print(f"ğŸ¤— Transformers ë²„ì „: {transformers_version}")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_gpu_memory()
    
    # Transformers ë²„ì „ í™•ì¸
    required_version = "4.37.0"
    current_version = version.parse(transformers_version)
    required_version_parsed = version.parse(required_version)
    
    if current_version < required_version_parsed:
        print(f"âŒ Transformers ë²„ì „ì´ ë¶€ì¡±í•©ë‹ˆë‹¤!")
        print(f"   í˜„ì¬ ë²„ì „: {transformers_version}")
        print(f"   í•„ìš” ë²„ì „: {required_version}+")
        return
    
    print(f"âœ… Transformers ë²„ì „ í™•ì¸: {transformers_version}")
    
    # 1. ë°ì´í„° ë³€í™˜
    print("\nğŸ“Š 1. ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ë° ë³€í™˜")
    converter = TtalkkacDatasetConverter()
    
    results_dir = os.getenv('RESULTS_DIR', os.path.join(PROJECT_ROOT, "ttalkkac_gold_standard_results_output"))
    gold_data = converter.load_gold_standard_data(results_dir)
    
    if not gold_data:
        print("âŒ ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(gold_data)}ê°œ")
    
    # ì‹¤ì œ í”„ë¡œì íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    training_data = converter.convert_to_training_format(gold_data)
    
    if not training_data:
        print("âŒ ë³€í™˜ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… í•™ìŠµ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(training_data)}ê°œ")
    
    # 2. íŒŒì¸íŠœë‹ ì„¤ì • ë° ì‹¤í–‰
    print("\nğŸ¤– 2. Qwen3 ëª¨ë¸ ì„¤ì • ë° LoRA íŒŒì¸íŠœë‹")
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"   GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    else:
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # íŒŒì¸íŠœë„ˆ ì´ˆê¸°í™”
    finetuner = QwenFineTuner("Qwen/Qwen3-4B-Thinking-2507")
    finetuner.data_converter = converter
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
    try:
        finetuner.setup_model_and_tokenizer()
    except RuntimeError as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    MAX_SEQ_LENGTH = int(os.getenv('MAX_SEQ_LENGTH', '12000'))
    VAL_SPLIT_RATIO = float(os.getenv('VAL_SPLIT_RATIO', '0.2'))
    train_dataset, val_dataset = finetuner.prepare_dataset(
        training_data, 
        max_length=MAX_SEQ_LENGTH,
        val_split=VAL_SPLIT_RATIO
    )
    
    if train_dataset is None:
        print("âŒ í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨")
        return
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    output_dir = f"./qwen3_4B_Thinking_lora_ttalkkac_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    trainer = finetuner.train(train_dataset, val_dataset, output_dir)
    
    print("\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir}")  

if __name__ == "__main__":
    main()