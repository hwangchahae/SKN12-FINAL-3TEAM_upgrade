# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import threading
import yaml

# Import prompt generation functions from ai-engine-dev modules
import sys
import os

# ai-engine-dev ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ai_engine_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'ai-engine-dev')
if ai_engine_path not in sys.path:
    sys.path.insert(0, ai_engine_path)

try:
    from prd_generation_prompts import (
        generate_notion_project_prompt,
        NOTION_PROJECT_SCHEMA
    )
    from meeting_analysis_prompts import (
        generate_meeting_analysis_user_prompt,
        MEETING_ANALYSIS_SCHEMA
    )
except ImportError as e:
    print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print(f"ğŸ“ ai-engine-dev ê²½ë¡œ: {ai_engine_path}")
    print("ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ ì •ì˜ (í´ë°±)
    def generate_notion_project_prompt(meeting_transcript: str) -> str:
        return f"""
ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„",
    "project_manager": "ë‹´ë‹¹ìëª…",
    "core_objectives": [
        "ëª©í‘œ 1: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 2: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 3: êµ¬ì²´ì ì¸ ëª©í‘œ"
    ],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´ ì„¤ëª…",
    "idea_description": "ì•„ì´ë””ì–´ì˜ ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ì  ì„¤ëª…",
    "execution_plan": "ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ ì¼ì •",
    "expected_effects": [
        "ê¸°ëŒ€íš¨ê³¼ 1: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 2: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 3: ìì„¸í•œ ì„¤ëª…"
    ]
}}
"""
    
    def generate_meeting_analysis_user_prompt(transcript: str, additional_context: str = "") -> str:
        context_section = f"\n\n**Additional Context:**\n{additional_context}" if additional_context else ""
        return f"""
Analyze the following meeting transcript and extract actionable tasks:

**Meeting Transcript:**
{transcript}
{context_section}

**Analysis Requirements:**
1. Identify all action items, decisions, and next steps
2. Extract or infer responsible parties (assignees)
3. Determine realistic deadlines based on context
4. Assess priority levels for each task
5. Create a comprehensive meeting summary
6. Identify key decisions and their implications

**Response Format:**
Provide a JSON response with the exact structure specified in the schema.
All text should be in Korean unless technical terms require English.
"""
    
    NOTION_PROJECT_SCHEMA = {}
    MEETING_ANALYSIS_SCHEMA = {}

# ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_config(config_path="config.yaml"):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# ì „ì—­ ì„¤ì • ë³€ìˆ˜
config = load_config()

# ì„¤ì • íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
if config and 'base_model' in config:
    model_path = config['base_model'].get('model_path', "Qwen/Qwen3-4B-AWQ")
    CHUNK_SIZE = config['base_model'].get('chunk_size', 5000)
    CHUNK_OVERLAP = config['base_model'].get('chunk_overlap', 512)
    TEMPERATURE = config['base_model'].get('temperature', 0.2)
    MAX_TOKENS = config['base_model'].get('max_tokens', 2048)
    TENSOR_PARALLEL_SIZE = config['base_model'].get('tensor_parallel_size', 1)
    GPU_MEMORY_UTILIZATION = config['base_model'].get('gpu_memory_utilization', 0.7)
    MAX_MODEL_LEN = config['base_model'].get('max_model_len', 16384)
else:
    # ê¸°ë³¸ê°’
    model_path = "Qwen/Qwen3-4B-AWQ"
    CHUNK_SIZE = 5000
    CHUNK_OVERLAP = 512
    TEMPERATURE = 0.2
    MAX_TOKENS = 2048
    TENSOR_PARALLEL_SIZE = 1
    GPU_MEMORY_UTILIZATION = 0.7
    MAX_MODEL_LEN = 16384

print(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € (í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ì´ˆê¸°í™”)
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        print(f"ğŸ”§ í”„ë¡œì„¸ìŠ¤ {os.getpid()}ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™”
        llm = LLM(
            model=model_path,
            quantization="awq_marlin" if "AWQ" in model_path else None,
            tensor_parallel_size=TENSOR_PARALLEL_SIZE,
            max_model_len=MAX_MODEL_LEN,
            gpu_memory_utilization=GPU_MEMORY_UTILIZATION,  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •
            trust_remote_code=True,
            enforce_eager=False,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        sampling_params = SamplingParams(
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            stop=None,
            skip_special_tokens=True,
        )
        print(f"âœ… í”„ë¡œì„¸ìŠ¤ {os.getpid()} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON íŒŒì¼ì„ ë¡œë“œ (qwen3_lora_meeting_generator_vllm.py ë°©ì‹)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not isinstance(data, list):
            print(f"âš ï¸  {file_path}ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return []
            
        # ê° í•­ëª©ì—ì„œ timestamp, speaker, text ì¶”ì¶œ
        processed_data = []
        for item in data:
            if isinstance(item, dict):
                processed_data.append({
                    "timestamp": item.get("timestamp", "Unknown"),
                    "speaker": item.get("speaker", "Unknown"),
                    "text": item.get("text", "")
                })
        
        return processed_data
                
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def chunk_text_simple(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœ ë¬¸ì ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ (qwen3_lora_meeting_generator_vllm.py ë°©ì‹)"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:]
        else:
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > start + chunk_size // 2:
                chunk = text[start:break_point + 1]
                end = break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    return chunks

def process_utterances_to_text(utterances, speakers_set=None):
    """ë°œí™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (qwen3 ë°©ì‹ê³¼ ë™ì¼)"""
    if not utterances:
        return "", []
        
    meeting_lines = []
    speakers = speakers_set if speakers_set else set()
    
    for item in utterances:
        timestamp = item.get('timestamp', 'Unknown')
        speaker = item.get('speaker', 'Unknown')
        text = item.get('text', '')
        speakers.add(speaker)
        meeting_lines.append(f"[{timestamp}] {speaker}: {text}")
    
    full_text = '\n'.join(meeting_lines)
    return full_text, list(speakers)

# generate_chunk_summary í•¨ìˆ˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ë¨
# ê°œë³„ ì²­í¬ ì²˜ë¦¬ ëŒ€ì‹  process_single_file_parallelì—ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬

def process_single_file_parallel(input_file_path, output_dir, model_used, folder_name, chunk_size=None, overlap=None):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë³„ë¡œ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹)"""
    from pathlib import Path
    
    # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
    rel_input_path = os.path.relpath(input_file_path, os.getcwd())
    print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {rel_input_path}")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    if llm is None:
        initialize_model()
    
    # ë°œí™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    utterances = load_json_file(input_file_path)
    if not utterances:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 1  # (ì„±ê³µ, ì‹¤íŒ¨) íŠœí”Œ ë°˜í™˜
    
    # ë°œí™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    full_text, speakers = process_utterances_to_text(utterances)
    
    if not full_text:
        print(f"âš ï¸  {input_file_path}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 1
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (ìƒëŒ€ ê²½ë¡œë¡œ ë³€ê²½)
    metadata = {
        "source_file": f"Raw_Data_val/{folder_name}/05_final_result.json",
        "utterance_count": len(utterances),
        "speakers": speakers,
        "original_length": len(full_text)
    }
    
    # ì²­í‚¹ íŒŒë¼ë¯¸í„° ì„¤ì • (ì¸ìë¡œ ë°›ê±°ë‚˜ ì „ì—­ ì„¤ì • ì‚¬ìš©)
    if chunk_size is None:
        chunk_size = CHUNK_SIZE
    if overlap is None:
        overlap = CHUNK_OVERLAP
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì²­í‚¹ ê²°ì •
    if len(full_text) > chunk_size:
        print(f"ğŸ“Š ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(full_text)}ì) - ì²­í‚¹ ì²˜ë¦¬")
        chunks = chunk_text_simple(full_text, chunk_size, overlap)
        print(f"ğŸ“š {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        metadata["chunking_info"] = {
            "is_chunked": True,
            "total_chunks": len(chunks)
        }
    else:
        print(f"ğŸ“„ ì§§ì€ í…ìŠ¤íŠ¸ ({len(full_text)}ì) - ë‹¨ì¼ ì²˜ë¦¬")
        chunks = [full_text]
        metadata["chunking_info"] = {
            "is_chunked": False,
            "total_chunks": 1
        }
    
    success_count = 0
    fail_count = 0
    output_path = Path(output_dir)
    total_chunks = len(chunks)
    
    # ëª¨ë“  ì²­í¬ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ì¤€ë¹„ (ë°°ì¹˜ ì²˜ë¦¬)
    all_prompts = []
    chunk_infos = []
    summary_accum = ""
    
    for chunk_idx, chunk_text in enumerate(chunks):
        # ì²­í¬ ì •ë³´ ì €ì¥ - folder_nameì´ ì´ë¯¸ result_ë¡œ ì‹œì‘í•¨
        if total_chunks == 1:
            chunk_dir = output_path / folder_name
            chunk_id = folder_name
        else:
            chunk_dir = output_path / f"{folder_name}_chunk_{chunk_idx+1}"
            chunk_id = f"{folder_name}_chunk_{chunk_idx+1}"
        
        chunk_infos.append({
            "chunk_dir": chunk_dir,
            "chunk_id": chunk_id,
            "chunk_idx": chunk_idx,
            "chunk_text": chunk_text,
            "summary_accum": summary_accum
        })
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        participants_str = ", ".join(speakers) if speakers else "ì•Œ ìˆ˜ ì—†ìŒ"
        meeting_transcript = f"""ì°¸ì—¬ì: {participants_str}

{chunk_text}"""
        
        system_prompt = """ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¸°íšì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‘ë‹µì€ ë°˜ë“œì‹œ ìš”ì²­ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”."""
        
        if chunk_idx == 0:
            # ì²« ë²ˆì§¸ ì²­í¬ëŠ” ë…¸ì…˜ í”„ë¡œì íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            user_prompt = generate_notion_project_prompt(meeting_transcript)
        else:
            # í›„ì† ì²­í¬ëŠ” íšŒì˜ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            additional_context = f"ì´ì „ ë¶„ì„ ê²°ê³¼:\n{summary_accum}"
            user_prompt = generate_meeting_analysis_user_prompt(chunk_text, additional_context)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        all_prompts.append(formatted_prompt)
        summary_accum += f"ì²­í¬ {chunk_idx+1} ì²˜ë¦¬ ì˜ˆì •\n"
    
    # ë°°ì¹˜ë¡œ ëª¨ë“  ì²­í¬ ì²˜ë¦¬
    print(f"ğŸš€ {len(all_prompts)}ê°œ ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    outputs = llm.generate(all_prompts, sampling_params)
    
    # ê²°ê³¼ ì €ì¥
    for idx, (output, chunk_info) in enumerate(zip(outputs, chunk_infos)):
        chunk_dir = chunk_info["chunk_dir"]
        chunk_id = chunk_info["chunk_id"]
        chunk_idx = chunk_info["chunk_idx"]
        
        chunk_dir.mkdir(parents=True, exist_ok=True)
        
        if output and output.outputs:
            result = output.outputs[0].text.strip()
            
            # JSON ì‘ë‹µ ì²˜ë¦¬ - raw_text ì—†ì´ JSONë§Œ ì¶”ì¶œ
            try:
                # <think> íƒœê·¸ê°€ ìˆìœ¼ë©´ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if "<think>" in result and "{" in result:
                    # <think> íƒœê·¸ ì´í›„ì˜ JSON ë¶€ë¶„ ì°¾ê¸°
                    json_start = result.find("{", result.find("</think>") if "</think>" in result else 0)
                    if json_start != -1:
                        # JSON ë ì°¾ê¸°
                        bracket_count = 0
                        json_end = json_start
                        for i, char in enumerate(result[json_start:], json_start):
                            if char == '{':
                                bracket_count += 1
                            elif char == '}':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    json_end = i + 1
                                    break
                        json_str = result[json_start:json_end]
                    else:
                        json_str = None
                elif "```json" in result:
                    start = result.find("```json") + 7
                    end = result.find("```", start)
                    if end != -1:
                        json_str = result[start:end].strip()
                    else:
                        json_str = result[start:].strip()
                elif result.startswith("{"):
                    json_str = result
                else:
                    json_str = None
                
                if json_str:
                    result_data = json.loads(json_str)
                else:
                    # raw_text ëŒ€ì‹  ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                    result_data = {
                        "project_name": "íšŒì˜ ê¸°ë°˜ í”„ë¡œì íŠ¸",
                        "project_purpose": "íšŒì˜ ë‚´ìš© ê¸°ë°˜ í”„ë¡œì íŠ¸ ì¶”ì§„",
                        "project_period": "ë¯¸ì •",
                        "project_manager": "ë¯¸ì •",
                        "core_objectives": [
                            "ëª©í‘œ 1: êµ¬ì²´ì ì¸ ëª©í‘œ",
                            "ëª©í‘œ 2: êµ¬ì²´ì ì¸ ëª©í‘œ",
                            "ëª©í‘œ 3: êµ¬ì²´ì ì¸ ëª©í‘œ"
                        ],
                        "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´ ì„¤ëª…",
                        "idea_description": "ì•„ì´ë””ì–´ì˜ ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ì  ì„¤ëª…",
                        "execution_plan": "ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ ì¼ì •",
                        "expected_effects": [
                            "ê¸°ëŒ€íš¨ê³¼ 1: ìì„¸í•œ ì„¤ëª…",
                            "ê¸°ëŒ€íš¨ê³¼ 2: ìì„¸í•œ ì„¤ëª…",
                            "ê¸°ëŒ€íš¨ê³¼ 3: ìì„¸í•œ ì„¤ëª…"
                        ]
                    }
            except (json.JSONDecodeError, Exception) as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                result_data = {
                    "project_name": "íšŒì˜ ê¸°ë°˜ í”„ë¡œì íŠ¸",
                    "project_purpose": "íšŒì˜ ë‚´ìš© ê¸°ë°˜ í”„ë¡œì íŠ¸ ì¶”ì§„",
                    "project_period": "ë¯¸ì •",
                    "project_manager": "ë¯¸ì •",
                    "core_objectives": [
                        "ëª©í‘œ 1: êµ¬ì²´ì ì¸ ëª©í‘œ",
                        "ëª©í‘œ 2: êµ¬ì²´ì ì¸ ëª©í‘œ",
                        "ëª©í‘œ 3: êµ¬ì²´ì ì¸ ëª©í‘œ"
                    ],
                    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´ ì„¤ëª…",
                    "idea_description": "ì•„ì´ë””ì–´ì˜ ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ì  ì„¤ëª…",
                    "execution_plan": "ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ ì¼ì •",
                    "expected_effects": [
                        "ê¸°ëŒ€íš¨ê³¼ 1: ìì„¸í•œ ì„¤ëª…",
                        "ê¸°ëŒ€íš¨ê³¼ 2: ìì„¸í•œ ì„¤ëª…",
                        "ê¸°ëŒ€íš¨ê³¼ 3: ìì„¸í•œ ì„¤ëª…"
                    ]
                }
            
            # ê²°ê³¼ ì €ì¥ (ì›ë˜ êµ¬ì¡°ëŒ€ë¡œ ë³µì›)
            chunk_result = {
                "id": chunk_id,
                "source_dir": folder_name,  # folder_nameì´ ì´ë¯¸ result_ë¡œ ì‹œì‘í•¨
                "notion_output": result_data,
                "metadata": {
                    **metadata,
                    "is_chunk": total_chunks > 1,
                    "chunk_index": chunk_idx + 1 if total_chunks > 1 else None,
                    "processing_date": datetime.now().isoformat()
                }
            }
            
            with open(chunk_dir / "result.json", 'w', encoding='utf-8') as f:
                json.dump(chunk_result, f, ensure_ascii=False, indent=2)
            
            success_count += 1
            if total_chunks > 1:
                print(f"âœ… ì²­í¬ {chunk_idx+1}/{total_chunks} ì €ì¥ ì™„ë£Œ: {chunk_dir.name}")
            else:
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {chunk_dir.name}")
        else:
            fail_count += 1
            if total_chunks > 1:
                print(f"âŒ ì²­í¬ {chunk_idx+1}/{total_chunks} ìƒì„± ì‹¤íŒ¨")
            else:
                print(f"âŒ ìƒì„± ì‹¤íŒ¨")
    
    return success_count, fail_count

# save_final_result_as_txt í•¨ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ (ê° ì²­í¬ë³„ë¡œ ê°œë³„ JSON ì €ì¥)
# qwen3_lora_meeting_generator_vllm.py ë°©ì‹ìœ¼ë¡œ ì €ì¥

def process_file_wrapper(args):
    """ThreadPoolExecutorë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    folder_name, folder_path, json_file, model_used = args
    
    try:
        # ì²­í‚¹ ì„¤ì • (qwen3_lora_meeting_generator_vllm.pyì™€ ë™ì¼í•˜ê²Œ)
        success_count, fail_count = process_single_file_parallel(
            json_file, 
            folder_path, 
            model_used, 
            folder_name,
            chunk_size=CHUNK_SIZE,  # ë¬¸ì ê¸°ë°˜ ì²­í‚¹ í¬ê¸°
            overlap=CHUNK_OVERLAP   # ì˜¤ë²„ë© í¬ê¸°
        )
        
        if success_count > 0:
            return (folder_name, True, f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}")
        else:
            return (folder_name, False, f"ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨ (ì´ {fail_count}ê°œ)")
    except Exception as e:
        return (folder_name, False, str(e))

def batch_process_folders_parallel(base_dir, model_used, output_base_dir=None, max_workers=None):
    """ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ í´ë” ì²˜ë¦¬ (vLLM ì•ˆì •ì„± í™•ë³´)
    
    Args:
        base_dir: ì…ë ¥ ë°ì´í„° ë””ë ‰í† ë¦¬
        model_used: ì‚¬ìš© ëª¨ë¸ëª…
        output_base_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì…ë ¥ ë””ë ‰í† ë¦¬ì™€ ë™ì¼í•œ ìœ„ì¹˜ì— ìƒì„±)
        max_workers: (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€)
    """
    
    if not os.path.exists(base_dir):
        # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
        rel_base_dir = os.path.relpath(base_dir, os.getcwd())
        print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {rel_base_dir}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_base_dir is None:
        # ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì— ê²°ê³¼ í´ë” ìƒì„±
        parent_dir = os.path.dirname(base_dir)
        output_base_dir = os.path.join(parent_dir, f"results_{model_used}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_base_dir, exist_ok=True)
    
    # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
    rel_output_dir = os.path.relpath(output_base_dir, os.getcwd())
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {rel_output_dir}")
    
    # í•˜ìœ„ í´ë”ë“¤ ì°¾ê¸°
    subfolders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            json_file = os.path.join(item_path, "05_final_result.json")
            if os.path.exists(json_file):
                # ì¶œë ¥ ê²½ë¡œë¥¼ output_base_dirë¡œ ë³€ê²½
                subfolders.append((item, output_base_dir, json_file, model_used))
            else:
                print(f"âš ï¸  {item} í´ë”ì— 05_final_result.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    if not subfolders:
        print(f"âŒ {base_dir}ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ì´ {len(subfolders)}ê°œ í´ë”ë¥¼ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
    for folder_name, _, _, _ in subfolders:
        print(f"  - {folder_name}")
    
    print(f"ğŸš€ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    success_count = 0
    failed_folders = []
    
    # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½ (vLLM ì•ˆì •ì„± í™•ë³´)
    total_chunks_processed = 0
    with tqdm(total=len(subfolders), desc="ğŸ“ ì „ì²´ í´ë” ì²˜ë¦¬", unit="folder") as pbar:
        for args in subfolders:
            folder_name = args[0]
            try:
                folder_name, success, message = process_file_wrapper(args)
                if success:
                    success_count += 1
                    # ì„±ê³µ/ì‹¤íŒ¨ ì²­í¬ ìˆ˜ íŒŒì‹±
                    if "ì„±ê³µ:" in message:
                        chunk_count = int(message.split("ì„±ê³µ: ")[1].split(",")[0])
                        total_chunks_processed += chunk_count
                else:
                    failed_folders.append((folder_name, message))
            except Exception as e:
                failed_folders.append((folder_name, str(e)))
            
            pbar.update(1)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µí•œ í´ë”: {success_count}/{len(subfolders)}")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ì´ ì²­í¬ ìˆ˜: {total_chunks_processed}")
    
    if failed_folders:
        print(f"\nâŒ ì‹¤íŒ¨í•œ í´ë”ë“¤:")
        for folder, error in failed_folders:
            print(f"  - {folder}: {error}")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ì„¤ì • íŒŒì¼ì—ì„œ ê²½ë¡œ ì½ê¸°
    if config and 'base_model' in config:
        model_used = config['base_model'].get('model_name', 'Qwen3_4B')
        base_directory = config['base_model'].get('input_dir', '../Raw_Data_val')
        output_directory = config['base_model'].get('output_dir', '../Pre_Training/4B_base_model_results')
    else:
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        model_used = "Qwen3_4B"  # ì¶œë ¥ í´ë”ëª…ì— ì‚¬ìš©ë  ì´ë¦„
        base_directory = "../Raw_Data_val"
        output_directory = "../Pre_Training/4B_base_model_results"
    
    print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {model_path} ëª¨ë¸ ì‚¬ìš©")
    print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {base_directory}")
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_directory}")
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not os.path.exists(base_directory):
        print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_directory}")
        print(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.path.relpath(os.getcwd(), '.')}")
        print(f"ğŸ’¡ config.yaml íŒŒì¼ì—ì„œ input_dir ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)
    
    # ìˆœì°¨ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    batch_process_folders_parallel(
        base_dir=base_directory, 
        model_used=model_used, 
        output_base_dir=output_directory
    )